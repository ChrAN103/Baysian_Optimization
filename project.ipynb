{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import GPyOpt\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "import random\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 5000 samples (reduced from 60000)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "epochs = 2  # Reduced for faster experimentation\n",
    "subset_size = 5000\n",
    "\n",
    "# Data transformation and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_full_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create a subset of the training data\n",
    "train_indices = np.random.choice(len(train_full_dataset), subset_size, replace=False)\n",
    "train_dataset = Subset(train_full_dataset, train_indices)\n",
    "\n",
    "print(f\"Training on {len(train_dataset)} samples (reduced from {len(train_full_dataset)})\")\n",
    "\n",
    "# Simple data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, conv1_channels=16, conv2_channels=32, dropout_rate=0.3, hidden_size=64):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, conv1_channels, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(conv1_channels, conv2_channels, 3, 1, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(conv2_channels * 14 * 14, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train(model, device, train_loader, optimizer, epoch, verbose=False):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
    "                  f' ({100. * batch_idx * len(data) / len(train_loader.dataset):.0f}%)]\\tLoss: {loss.item():.4f}')\n",
    "\n",
    "# Testing function\n",
    "def test(model, device, test_loader, verbose=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, '\n",
    "              f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "def evaluate_params(learning_rate, batch_size, dropout_rate, conv1_channels, conv2_channels, hidden_size):\n",
    "    \"\"\"Evaluate a set of parameters and return accuracy\"\"\"\n",
    "    # Convert parameters to appropriate types\n",
    "    batch_size = int(batch_size)\n",
    "    conv1_channels = int(conv1_channels)\n",
    "    conv2_channels = int(conv2_channels)\n",
    "    hidden_size = int(hidden_size)\n",
    "    \n",
    "    # Create model with the given hyperparameters\n",
    "    model = Net(\n",
    "        conv1_channels=conv1_channels, \n",
    "        conv2_channels=conv2_channels, \n",
    "        dropout_rate=dropout_rate, \n",
    "        hidden_size=hidden_size\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer with the given learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create data loaders with the given batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Train for a fixed number of epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, verbose=False)\n",
    "    \n",
    "    # Get final test accuracy\n",
    "    accuracy = test(model, device, test_loader, verbose=False)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Objective function for GPyOpt\n",
    "def objective_function(x, acq_name=None):\n",
    "    \"\"\"\n",
    "    Function to be optimized by GPyOpt's Bayesian optimization.\n",
    "    Takes a numpy array of parameters and returns the negative accuracy (to minimize).\n",
    "    \"\"\"\n",
    "    learning_rate = float(x[0][0])\n",
    "    batch_size = float(x[0][1])  # GPyOpt will handle converting to appropriate values\n",
    "    dropout_rate = float(x[0][2])\n",
    "    conv1_channels = float(x[0][3])\n",
    "    conv2_channels = float(x[0][4])\n",
    "    hidden_size = float(x[0][5])\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'conv1_channels': conv1_channels,\n",
    "        'conv2_channels': conv2_channels,\n",
    "        'hidden_size': hidden_size,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTesting {acq_name} with params: {params}\")\n",
    "    \n",
    "    # Evaluate parameters\n",
    "    accuracy = evaluate_params(\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        conv1_channels=conv1_channels,\n",
    "        conv2_channels=conv2_channels,\n",
    "        hidden_size=hidden_size\n",
    "    )\n",
    "    \n",
    "    print(f\"{acq_name} Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Store result\n",
    "    if acq_name not in all_results:\n",
    "        all_results[acq_name] = []\n",
    "    \n",
    "    all_results[acq_name].append({\n",
    "        'iteration': len(all_results[acq_name]) + 1,\n",
    "        'accuracy': accuracy,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'conv1_channels': conv1_channels,\n",
    "        'conv2_channels': conv2_channels,\n",
    "        'hidden_size': hidden_size\n",
    "    })\n",
    "    \n",
    "    # Return negative accuracy for minimization\n",
    "    return -accuracy\n",
    "\n",
    "def run_random_search(bounds, n_iter=20):\n",
    "    \"\"\"Run random search for comparison\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running Random Search for {n_iter} iterations\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    acq_name = \"Random\"\n",
    "    all_results[acq_name] = []\n",
    "    \n",
    "    # Extract domain ranges\n",
    "    param_ranges = []\n",
    "    for bound in bounds:\n",
    "        if bound['type'] == 'continuous':\n",
    "            param_ranges.append((bound['domain'][0], bound['domain'][1]))\n",
    "        else:  # discrete\n",
    "            param_ranges.append(bound['domain'])\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Sample random parameters\n",
    "        params = []\n",
    "        for j, param_range in enumerate(param_ranges):\n",
    "            if isinstance(param_range, tuple) and len(param_range) == 2:\n",
    "                # Continuous parameter\n",
    "                params.append(np.random.uniform(param_range[0], param_range[1]))\n",
    "            else:\n",
    "                # Discrete parameter\n",
    "                params.append(np.random.choice(param_range))\n",
    "        \n",
    "        # Evaluate parameters\n",
    "        accuracy = evaluate_params(\n",
    "            learning_rate=params[0],\n",
    "            batch_size=params[1],\n",
    "            dropout_rate=params[2],\n",
    "            conv1_channels=params[3],\n",
    "            conv2_channels=params[4],\n",
    "            hidden_size=params[5]\n",
    "        )\n",
    "        \n",
    "        print(f\"Random Search Iteration {i+1}/{n_iter}: Accuracy = {accuracy:.2f}%\")\n",
    "        \n",
    "        all_results[acq_name].append({\n",
    "            'iteration': i + 1,\n",
    "            'accuracy': accuracy,\n",
    "            'learning_rate': params[0],\n",
    "            'batch_size': params[1],\n",
    "            'dropout_rate': params[2],\n",
    "            'conv1_channels': params[3],\n",
    "            'conv2_channels': params[4],\n",
    "            'hidden_size': params[5]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acquisition_comparison(all_results, max_iter=None):\n",
    "    \"\"\"Plot the performance of different acquisition functions\"\"\"\n",
    "    # Create plots directory\n",
    "    import os\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot the maximum accuracy achieved so far for each acquisition function\n",
    "    for acq_name, results in all_results.items():\n",
    "        iterations = []\n",
    "        max_accuracies = []\n",
    "        current_max = 0\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        if max_iter:\n",
    "            df = df[df['iteration'] <= max_iter]\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            current_max = max(current_max, row['accuracy'])\n",
    "            iterations.append(row['iteration'])\n",
    "            max_accuracies.append(current_max)\n",
    "        \n",
    "        plt.plot(iterations, max_accuracies, '-o', label=acq_name, linewidth=2, markersize=6)\n",
    "    \n",
    "    plt.xlabel('Iteration', fontsize=14)\n",
    "    plt.ylabel('Max Accuracy (%)', fontsize=14)\n",
    "    plt.title('Comparison of Acquisition Functions for Bayesian Optimization', fontsize=16)\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('plots/acquisition_function_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Also save results as CSV\n",
    "    all_df = pd.concat([pd.DataFrame(results).assign(method=name) for name, results in all_results.items()])\n",
    "    all_df.to_csv('plots/all_optimization_results.csv', index=False)\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter bounds for GPyOpt Bayesian optimization\n",
    "bounds = [\n",
    "    {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.0001, 0.01)},\n",
    "    {'name': 'batch_size', 'type': 'discrete', 'domain': (32, 64, 128, 256, 512)},\n",
    "    {'name': 'dropout_rate', 'type': 'continuous', 'domain': (0.1, 0.6)},\n",
    "    {'name': 'conv1_channels', 'type': 'discrete', 'domain': (8, 16, 24, 32)},\n",
    "    {'name': 'conv2_channels', 'type': 'discrete', 'domain': (16, 32, 48, 64)},\n",
    "    {'name': 'hidden_size', 'type': 'discrete', 'domain': (32, 64, 96, 128)}\n",
    "]\n",
    "\n",
    "# Number of iterations for each method\n",
    "n_iterations = 16\n",
    "# Number of initial points (for BO methods)\n",
    "initial_points = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running Random Search for 16 iterations\n",
      "==================================================\n",
      "Random Search Iteration 1/16: Accuracy = 95.66%\n",
      "Random Search Iteration 2/16: Accuracy = 79.57%\n",
      "Random Search Iteration 3/16: Accuracy = 96.67%\n",
      "Random Search Iteration 4/16: Accuracy = 95.74%\n",
      "Random Search Iteration 5/16: Accuracy = 90.46%\n",
      "Random Search Iteration 6/16: Accuracy = 90.22%\n",
      "Random Search Iteration 7/16: Accuracy = 92.42%\n",
      "Random Search Iteration 8/16: Accuracy = 94.74%\n",
      "Random Search Iteration 9/16: Accuracy = 96.53%\n",
      "Random Search Iteration 10/16: Accuracy = 95.38%\n",
      "Random Search Iteration 11/16: Accuracy = 90.13%\n",
      "Random Search Iteration 12/16: Accuracy = 89.16%\n",
      "Random Search Iteration 13/16: Accuracy = 93.00%\n",
      "Random Search Iteration 14/16: Accuracy = 95.22%\n",
      "Random Search Iteration 15/16: Accuracy = 10.09%\n",
      "Random Search Iteration 16/16: Accuracy = 95.92%\n"
     ]
    }
   ],
   "source": [
    "run_random_search(bounds, n_iter=n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting Bayesian Optimization with EI acquisition...\n",
      "==================================================\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.007677101016761654, 'batch_size': 512.0, 'dropout_rate': 0.18213771275894078, 'conv1_channels': 24.0, 'conv2_channels': 64.0, 'hidden_size': 32.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 92.87%\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.001481378525766465, 'batch_size': 32.0, 'dropout_rate': 0.4558702638139591, 'conv1_channels': 32.0, 'conv2_channels': 48.0, 'hidden_size': 64.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 96.15%\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.004669144408589858, 'batch_size': 512.0, 'dropout_rate': 0.23874732878258523, 'conv1_channels': 8.0, 'conv2_channels': 48.0, 'hidden_size': 128.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 91.52%\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.005084695737957518, 'batch_size': 128.0, 'dropout_rate': 0.41232746708448575, 'conv1_channels': 16.0, 'conv2_channels': 48.0, 'hidden_size': 64.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 95.43%\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.003117622164972596, 'batch_size': 32.0, 'dropout_rate': 0.26129487125359585, 'conv1_channels': 8.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 95.91%\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.0014895830885248978, 'batch_size': 32.0, 'dropout_rate': 0.4557781221019054, 'conv1_channels': 32.0, 'conv2_channels': 48.0, 'hidden_size': 64.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 95.84%\n",
      "num acquisition: 1, time elapsed: 4.29s\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.0014854696634469641, 'batch_size': 32.0, 'dropout_rate': 0.45582415743700105, 'conv1_channels': 32.0, 'conv2_channels': 48.0, 'hidden_size': 64.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 96.35%\n",
      "num acquisition: 2, time elapsed: 8.59s\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.01, 'batch_size': 32.0, 'dropout_rate': 0.6, 'conv1_channels': 32.0, 'conv2_channels': 64.0, 'hidden_size': 32.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 92.68%\n",
      "num acquisition: 3, time elapsed: 12.89s\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.006884610383369684, 'batch_size': 32.0, 'dropout_rate': 0.23700356006192042, 'conv1_channels': 8.0, 'conv2_channels': 16.0, 'hidden_size': 128.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 95.12%\n",
      "num acquisition: 4, time elapsed: 16.80s\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.006391854018099048, 'batch_size': 32.0, 'dropout_rate': 0.16513586059475047, 'conv1_channels': 32.0, 'conv2_channels': 16.0, 'hidden_size': 32.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 95.84%\n",
      "num acquisition: 5, time elapsed: 21.00s\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.008634339523615793, 'batch_size': 32.0, 'dropout_rate': 0.1347310125535736, 'conv1_channels': 32.0, 'conv2_channels': 16.0, 'hidden_size': 32.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 10.09%\n",
      "num acquisition: 6, time elapsed: 25.00s\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.004198997597007677, 'batch_size': 512.0, 'dropout_rate': 0.5168807804510326, 'conv1_channels': 8.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 82.34%\n",
      "num acquisition: 7, time elapsed: 28.26s\n",
      "\n",
      "Testing Bayesian Optimization (BO) EI with params: {'learning_rate': 0.00263365405895565, 'batch_size': 512.0, 'dropout_rate': 0.5407718562335703, 'conv1_channels': 8.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "Bayesian Optimization (BO) EI Accuracy: 92.15%\n",
      "num acquisition: 8, time elapsed: 31.53s\n",
      "\n",
      "==================================================\n",
      "Starting Bayesian Optimization with MPI acquisition...\n",
      "==================================================\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.009653570608249087, 'batch_size': 512.0, 'dropout_rate': 0.49937314069775784, 'conv1_channels': 8.0, 'conv2_channels': 32.0, 'hidden_size': 96.0}\n",
      "BO MPI Accuracy: 91.83%\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.0002150769201787491, 'batch_size': 128.0, 'dropout_rate': 0.21252176961062016, 'conv1_channels': 32.0, 'conv2_channels': 16.0, 'hidden_size': 32.0}\n",
      "BO MPI Accuracy: 85.15%\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.004930772880186737, 'batch_size': 256.0, 'dropout_rate': 0.16900157433386834, 'conv1_channels': 32.0, 'conv2_channels': 64.0, 'hidden_size': 32.0}\n",
      "BO MPI Accuracy: 93.81%\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.00045857684681944425, 'batch_size': 128.0, 'dropout_rate': 0.2060103456236316, 'conv1_channels': 24.0, 'conv2_channels': 16.0, 'hidden_size': 96.0}\n",
      "BO MPI Accuracy: 90.87%\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.008783875305394812, 'batch_size': 128.0, 'dropout_rate': 0.13916402805932146, 'conv1_channels': 16.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO MPI Accuracy: 95.52%\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.00813621381393242, 'batch_size': 128.0, 'dropout_rate': 0.3107869394799737, 'conv1_channels': 16.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO MPI Accuracy: 91.68%\n",
      "num acquisition: 1, time elapsed: 3.41s\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.00847078774771451, 'batch_size': 128.0, 'dropout_rate': 0.21991860772264485, 'conv1_channels': 16.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO MPI Accuracy: 95.47%\n",
      "num acquisition: 2, time elapsed: 6.63s\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.008466622813094024, 'batch_size': 128.0, 'dropout_rate': 0.22125930247852724, 'conv1_channels': 16.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO MPI Accuracy: 10.28%\n",
      "num acquisition: 3, time elapsed: 9.96s\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.006003890471990267, 'batch_size': 512.0, 'dropout_rate': 0.2613225741119015, 'conv1_channels': 32.0, 'conv2_channels': 48.0, 'hidden_size': 128.0}\n",
      "BO MPI Accuracy: 89.80%\n",
      "num acquisition: 4, time elapsed: 13.30s\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.004849331874465917, 'batch_size': 512.0, 'dropout_rate': 0.39759298577762825, 'conv1_channels': 32.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "BO MPI Accuracy: 93.72%\n",
      "num acquisition: 5, time elapsed: 16.46s\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.003830611373154835, 'batch_size': 512.0, 'dropout_rate': 0.5668477890252687, 'conv1_channels': 32.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "BO MPI Accuracy: 93.04%\n",
      "num acquisition: 6, time elapsed: 19.92s\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.00847049332955155, 'batch_size': 512.0, 'dropout_rate': 0.5755366720578856, 'conv1_channels': 32.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "BO MPI Accuracy: 92.36%\n",
      "num acquisition: 7, time elapsed: 23.26s\n",
      "\n",
      "Testing BO MPI with params: {'learning_rate': 0.0029672345254125035, 'batch_size': 512.0, 'dropout_rate': 0.5951044570295141, 'conv1_channels': 32.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "BO MPI Accuracy: 92.28%\n",
      "num acquisition: 8, time elapsed: 26.39s\n",
      "\n",
      "==================================================\n",
      "Starting Bayesian Optimization with LCB acquisition...\n",
      "==================================================\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.006528444744926829, 'batch_size': 128.0, 'dropout_rate': 0.5145394818517125, 'conv1_channels': 8.0, 'conv2_channels': 64.0, 'hidden_size': 32.0}\n",
      "BO LCB Accuracy: 93.66%\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.0009166152620610717, 'batch_size': 512.0, 'dropout_rate': 0.17778120036884124, 'conv1_channels': 24.0, 'conv2_channels': 32.0, 'hidden_size': 96.0}\n",
      "BO LCB Accuracy: 87.28%\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.008601339005559245, 'batch_size': 64.0, 'dropout_rate': 0.3237275676544705, 'conv1_channels': 32.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO LCB Accuracy: 93.20%\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.009934070741618697, 'batch_size': 64.0, 'dropout_rate': 0.5623781265823423, 'conv1_channels': 8.0, 'conv2_channels': 32.0, 'hidden_size': 96.0}\n",
      "BO LCB Accuracy: 91.76%\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.0014326606202858239, 'batch_size': 128.0, 'dropout_rate': 0.5660408117343628, 'conv1_channels': 24.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO LCB Accuracy: 91.36%\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.008601963370793555, 'batch_size': 64.0, 'dropout_rate': 0.32361260987872004, 'conv1_channels': 32.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO LCB Accuracy: 11.35%\n",
      "num acquisition: 1, time elapsed: 3.73s\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.008601410671688372, 'batch_size': 64.0, 'dropout_rate': 0.32371517664758603, 'conv1_channels': 32.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO LCB Accuracy: 10.09%\n",
      "num acquisition: 2, time elapsed: 7.50s\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.008601104652231866, 'batch_size': 64.0, 'dropout_rate': 0.323717114125633, 'conv1_channels': 32.0, 'conv2_channels': 32.0, 'hidden_size': 32.0}\n",
      "BO LCB Accuracy: 87.66%\n",
      "num acquisition: 3, time elapsed: 11.13s\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.0001, 'batch_size': 128.0, 'dropout_rate': 0.1, 'conv1_channels': 8.0, 'conv2_channels': 64.0, 'hidden_size': 32.0}\n",
      "BO LCB Accuracy: 65.88%\n",
      "num acquisition: 4, time elapsed: 14.61s\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.008122679673203518, 'batch_size': 512.0, 'dropout_rate': 0.5591757616028913, 'conv1_channels': 8.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "BO LCB Accuracy: 92.65%\n",
      "num acquisition: 5, time elapsed: 17.99s\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.005679106444239976, 'batch_size': 512.0, 'dropout_rate': 0.5987637066884596, 'conv1_channels': 8.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "BO LCB Accuracy: 90.93%\n",
      "num acquisition: 6, time elapsed: 21.18s\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.005300938708689816, 'batch_size': 512.0, 'dropout_rate': 0.39921458300644785, 'conv1_channels': 8.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "BO LCB Accuracy: 91.77%\n",
      "num acquisition: 7, time elapsed: 24.31s\n",
      "\n",
      "Testing BO LCB with params: {'learning_rate': 0.005055688801115027, 'batch_size': 512.0, 'dropout_rate': 0.6, 'conv1_channels': 8.0, 'conv2_channels': 64.0, 'hidden_size': 128.0}\n",
      "BO LCB Accuracy: 91.61%\n",
      "num acquisition: 8, time elapsed: 27.60s\n",
      "\n",
      "Plotting comparison of acquisition functions...\n",
      "\n",
      "Best results by method:\n",
      "Random: 96.67% (iteration 3)\n",
      "Bayesian Optimization (BO) EI: 96.35% (iteration 12)\n",
      "BO MPI: 95.52% (iteration 5)\n",
      "BO LCB: 93.66% (iteration 1)\n",
      "\n",
      "Results saved to 'plots' directory.\n"
     ]
    }
   ],
   "source": [
    "# Define acquisition functions to test\n",
    "acquisition_functions = [\n",
    "    ('Bayesian Optimization (BO) EI', 'EI'),    # Expected Improvement\n",
    "    ('BO MPI', 'MPI'),  # Maximum Probability of Improvement\n",
    "    ('BO LCB', 'LCB')   # Lower Confidence Bound\n",
    "]\n",
    "\n",
    "# Run Bayesian optimization with each acquisition function\n",
    "for name, acq_type in acquisition_functions:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting Bayesian Optimization with {acq_type} acquisition...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Define objective function with the current acquisition name\n",
    "    def specific_objective(x):\n",
    "        return objective_function(x, acq_name=name)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=specific_objective,\n",
    "        domain=bounds,\n",
    "        model_type='GP',\n",
    "        acquisition_type=acq_type,\n",
    "        acquisition_jitter=0.01,\n",
    "        exact_feval=False,\n",
    "        maximize=False,\n",
    "        verbosity=True,\n",
    "        normalize_Y=False\n",
    "    )\n",
    "    \n",
    "    # Run optimization with initial exploration phase\n",
    "    if initial_points > 0:\n",
    "        optimizer.run_optimization(max_iter=0, verbosity=True, evaluations_file=f'plots/{name}_evaluations.csv')\n",
    "        # Continue with optimization phase\n",
    "        optimizer.run_optimization(max_iter=n_iterations-initial_points, verbosity=True, evaluations_file=f'plots/{name}_evaluations.csv')\n",
    "    else:\n",
    "        optimizer.run_optimization(max_iter=n_iterations, verbosity=True, evaluations_file=f'plots/{name}_evaluations.csv')\n",
    "\n",
    "# Plot comparison of all methods\n",
    "print(\"\\nPlotting comparison of acquisition functions...\")\n",
    "all_df = plot_acquisition_comparison(all_results)\n",
    "\n",
    "# Print best result for each method\n",
    "print(\"\\nBest results by method:\")\n",
    "for method, results in all_results.items():\n",
    "    best_result = max(results, key=lambda x: x['accuracy'])\n",
    "    print(f\"{method}: {best_result['accuracy']:.2f}% (iteration {best_result['iteration']})\")\n",
    "\n",
    "print(\"\\nResults saved to 'plots' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
